import sys
sys.path.append("./")
import cv2
import numpy as np
from PIL import ImageFont, ImageDraw, Image
from mss import mss
import time
import json
import torch
from torchvision import transforms
from models.maple_models import ResNetClassifier, EfficientNetClassifier

# ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
num_classes = 513
model = EfficientNetClassifier(num_classes=num_classes)
model.load_state_dict(torch.load("ckpt/test11.pt",map_location=torch.device('cpu')))
model.eval()

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

with open("inference/database_idx.json",encoding="UTF-8") as f:
    database_idx = json.load(f)

# === ë§ˆìš°ìŠ¤ë¡œ ì˜ì—­ ì„ íƒì„ ìœ„í•œ ë³€ìˆ˜ ===
selecting = False
start_point = (-1, -1)
end_point = (-1, -1)
region_selected = False

def mouse_callback(event, x, y, flags, param):
    global selecting, start_point, end_point, region_selected
    if event == cv2.EVENT_LBUTTONDOWN:
        selecting = True
        start_point = (x, y)
    elif event == cv2.EVENT_MOUSEMOVE and selecting:
        end_point = (x, y)
    elif event == cv2.EVENT_LBUTTONUP:
        selecting = False
        end_point = (x, y)
        region_selected = True


# === ëª¨ë‹ˆí„° ì„ íƒ ===
sct = mss()
print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë‹ˆí„° ëª©ë¡:")
for i, mon in enumerate(sct.monitors):
    print(f"Monitor {i}: {mon}")
selected_monitor_index = int(input("ì‚¬ìš©í•  ëª¨ë‹ˆí„° ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: "))
screen = sct.monitors[selected_monitor_index]

# === ì„ íƒí•œ ëª¨ë‹ˆí„° ìº¡ì²˜í•´ì„œ ë°°ê²½ ì´ë¯¸ì§€ë¡œ ì‚¬ìš© ===
screenshot = sct.grab(screen)
screen_img = np.array(screenshot)[:, :, :3].copy()

# === ë§ˆìš°ìŠ¤ë¡œ ì˜ì—­ ì„ íƒ ===
cv2.namedWindow("Select Region")
cv2.setMouseCallback("Select Region", mouse_callback)

print("ğŸ“Œ í™”ë©´ì—ì„œ ë§ˆìš°ìŠ¤ë¡œ ì˜ì—­ì„ ë“œë˜ê·¸í•˜ê³ , 's' í‚¤ë¥¼ ëˆŒëŸ¬ ì„ íƒì„ ì™„ë£Œí•˜ì„¸ìš”.")

while True:
    temp = screen_img.copy()
    if selecting or region_selected:
        cv2.rectangle(temp, start_point, end_point, (0, 255, 0), 2)
    cv2.imshow("Select Region", temp)
    key = cv2.waitKey(1)
    if key == ord('s') and region_selected:
        break
cv2.destroyWindow("Select Region")

# === ì„ íƒí•œ ì˜ì—­ ì •ë³´ ê³„ì‚° ===
left = min(start_point[0], end_point[0]) + screen["left"]
top = min(start_point[1], end_point[1]) + screen["top"]
width = abs(end_point[0] - start_point[0])
height = abs(end_point[1] - start_point[1])
monitor = {"top": top, "left": left, "width": width, "height": height}

print(f"âœ… ì„ íƒëœ ì˜ì—­: {monitor}")

# === ì‹¤ì‹œê°„ ë¶„ë¥˜ ì‹œì‘ ===
font = ImageFont.truetype("inference/NanumHumanHeavy.ttf", 20)
while True:
    screenshot = sct.grab(monitor)
    img = np.array(screenshot)[:, :, :3].copy()
    
    pil_img = Image.fromarray(img)
    tensor_img = transform(pil_img)
    
    pred = model(tensor_img.unsqueeze(0))
    pred_class = pred.argmax()

    most_similar_index = int(pred_class)
    
    label = f"{database_idx[str(most_similar_index)]}"

    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    draw.text((50, 50), label, font=font, fill=(255, 0, 0))
    img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    cv2.imshow("Live Classification", img)
    time.sleep(1)
    
    if cv2.waitKey(1) == ord("q"):
        break

cv2.destroyAllWindows()
